{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-starter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvm82yKXnXW_"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3xb-E8SniCa",
        "outputId": "a0cc9a30-ecd5-4307-94cb-4edb50752696"
      },
      "source": [
        "print(nltk.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwvsBentoSZs"
      },
      "source": [
        "Convert to Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN5AJqFonmRb",
        "outputId": "2b107fe1-a62e-46c1-ddff-8d88e8037690"
      },
      "source": [
        "#nltk.download('punkt')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgV8websntYI"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb1EJMghn15-",
        "outputId": "1ddb28c0-f9fa-41ba-aeb4-b907c346ba15"
      },
      "source": [
        "print(word_tokenize(\"You should visit Malawi sometime,it's a beautiful country.\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['You', 'should', 'visit', 'Malawi', 'sometime', ',', 'it', \"'s\", 'a', 'beautiful', 'country', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z5fPCJRoWHQ"
      },
      "source": [
        "Convert to base forms \n",
        "\n",
        "1. Stemming \n",
        "2. Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpACBnW-pRII"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yL5YqL-oZB-",
        "outputId": "f9f0f408-e65c-4784-e46d-a6a5fbbe476c"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer \n",
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem(\"going\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyGAU1SDpAgt",
        "outputId": "9e41b9e5-6727-4061-d697-f9e857e5de00"
      },
      "source": [
        "#weakness of stemming\n",
        "print(stemmer.stem(\"constitutes\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "constitut\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAoKT2BBpb5V"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw4Wo5YXpfJZ",
        "outputId": "99af8526-6d15-4a45-9567-13c83d704645"
      },
      "source": [
        "#nltk.download('wordnet')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKVIzOE9plo9"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "lem = WordNetLemmatizer()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLBupiStp1nx",
        "outputId": "46772099-bad9-428b-ecd9-a2fdba8b57c6"
      },
      "source": [
        "#corectly gives base word\n",
        "print(lem.lemmatize('constitutes', 'v'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "constitute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ZarmXSqOZe",
        "outputId": "18644c17-5ede-47d4-c5ae-3cf2f9447b80"
      },
      "source": [
        "#nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDzqZ5udqZxE",
        "outputId": "202e095f-7072-4252-df7a-239d74e3b4bf"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "sample = \"Hi, this is a nice hotel.\"\n",
        "print(pos_tag(word_tokenize(sample)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Hi', 'NNP'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('nice', 'JJ'), ('hotel', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XukpdTRPr2R0",
        "outputId": "7af95b37-7d85-49f8-943d-e1f67a1dc5b0"
      },
      "source": [
        "def lemmatize_tokens(stentence):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = []\n",
        "  for word, tag in pos_tag(stentence):\n",
        "    if tag.startswith('NN'):\n",
        "      pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "      pos = 'v'\n",
        "    else:\n",
        "      pos = 'a'\n",
        "    lemmatized_tokens.append(lemmatizer.lemmatize(word, pos))\n",
        "  return lemmatized_tokens\n",
        "\n",
        "sample = \"Legal authority constitutes all magistrates.\"\n",
        "print(lemmatize_tokens(word_tokenize(sample)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Legal', 'authority', 'constitute', 'all', 'magistrate', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ9aFr7Ls5Xn"
      },
      "source": [
        "Data cleaning \n",
        "1. Remove punctuation\n",
        "2. Reomove Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqfBINMGthcF"
      },
      "source": [
        "Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tViSDvLEs-yf",
        "outputId": "ec606280-9a42-4a1c-8606-0843b28826eb"
      },
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "her8QaIKtUfO",
        "outputId": "26fffcae-b40f-4deb-afa9-aa2ebf1cf56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "for token in tokens:\n",
        "  if token in string.punctuation:\n",
        "    # Do something"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-ad643f268c47>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    # Do something\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gmv_B3v42Kj"
      },
      "source": [
        "Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgD-wZoa4avJ",
        "outputId": "a5d08415-2177-4a3c-a0c4-32f90d1546c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWB8zsN_474f"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAO2oM5n5IGW"
      },
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q_nyz325jkA",
        "outputId": "1d2ffe58-4ece-413c-8b26-79feea623728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def clean_data(tokens, stop_words = ()):\n",
        "\n",
        "  cleaned_tokens = []\n",
        "\n",
        "  for token, tag in pos_tag(tokens):\n",
        "    if tag.startswith(\"NN\"):\n",
        "      pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "      pos = 'v'\n",
        "    else:\n",
        "      pos = 'a'\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "    if token not in string.punctuation and token.lower() not in stop_words:\n",
        "      cleaned_tokens.append(token)\n",
        "  return cleaned_tokens\n",
        "\n",
        "sample = \"The quick brown fox jumps over the lazy dog.\"\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "clean_data(word_tokenize(sample), stop_words)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}